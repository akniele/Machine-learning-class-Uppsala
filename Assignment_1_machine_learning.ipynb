{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task\n",
        "This assignment is about re-implementing a pipeline for binary sentiment classification (to classify film reviews as either positive or negative). A working pipeline written with sklearn is given. The task is to re-implement the feature encoding, model, a learning algorithm for the model parameters, and a prediction algorithm."
      ],
      "metadata": {
        "id": "I_yLv4KCPGKR"
      },
      "id": "I_yLv4KCPGKR"
    },
    {
      "cell_type": "markdown",
      "id": "1e0786ed",
      "metadata": {
        "id": "1e0786ed"
      },
      "source": [
        "# Part 1: Parsing the dataset\n",
        "I use *tarfile* to extract the data from the tar.gz file. After originally reading all the reviews and their corresponding labels (-1 or 1) into lists, I convert all the lists to np-arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adbf9906",
      "metadata": {
        "id": "adbf9906",
        "outputId": "f155fb75-a9fa-443b-b377-9e58b30869ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('data.tar.gz', <http.client.HTTPMessage at 0x20286de4a00>)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import urllib.request\n",
        "url = 'http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz'\n",
        "filename = 'data.tar.gz'\n",
        "urllib.request.urlretrieve(url, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e824174",
      "metadata": {
        "id": "9e824174"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# extract tar gz \n",
        "file = tarfile.open('data.tar.gz')\n",
        "file.extractall()\n",
        "file.close()\n",
        "\n",
        "# make lists of file names, one for pos and one for neg\n",
        "path_neg = './txt_sentoken/neg'\n",
        "dir_list_neg = os.listdir(path_neg)\n",
        "path_pos = './txt_sentoken/pos'\n",
        "dir_list_pos = os.listdir(path_pos)\n",
        "\n",
        "# create empty lists X_raw_pos and X_raw_neg \n",
        "X_raw_pos = []\n",
        "X_raw_neg = []\n",
        "\n",
        "# iterate over file-name-lists, add file contents to X_raw_neg and X_raw_pos\n",
        "for dir in dir_list_neg:\n",
        "    new_file = open(f'./txt_sentoken/neg/{dir}', \"r\")\n",
        "    file_content = new_file.read()\n",
        "    X_raw_neg.append(file_content)\n",
        "\n",
        "for dir in dir_list_pos:\n",
        "    new_file = open(f'./txt_sentoken/pos/{dir}', \"r\")\n",
        "    file_content = new_file.read()\n",
        "    X_raw_pos.append(file_content)\n",
        "\n",
        "#X_raw will be needed to create the vocabulary\n",
        "X_raw = X_raw_neg + X_raw_pos\n",
        "\n",
        "# X_raw_train is the first 800 negative and the first 800 positive reviews\n",
        "# X_raw_test is the last 200 negative and the last 200 positive reviews\n",
        "X_raw_train = X_raw_neg[:800] + X_raw_pos[:800]\n",
        "X_raw_test = X_raw_neg[800:] + X_raw_pos[800:]\n",
        "\n",
        "# prepare y, y_train and y_test\n",
        "y_neg = [-1 for i in range(len(dir_list_neg))]\n",
        "y_pos = [1 for i in range(len(dir_list_pos))]\n",
        "y = y_neg + y_pos\n",
        "y_train = [-1 for i in range(800)] + [1 for i in range(800)]\n",
        "y_test = [-1 for i in range(200)] + [1 for i in range(200)]\n",
        "\n",
        "# turn y, y_train and y_test into numpy arrays\n",
        "y = np.array(y)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8d8be07",
      "metadata": {
        "id": "c8d8be07"
      },
      "source": [
        "# Part 2: Feature extraction\n",
        "I tokenise the reviews by simply splitting them at white spaces. I create a dictionary that stores the vocabulary words as keys and their position in the ordered vocabulary as values. This allows for a faster lookup when creating the bag-of-words vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "705c4e5e",
      "metadata": {
        "id": "705c4e5e"
      },
      "outputs": [],
      "source": [
        "#create the vocabulary by iterating over the words in the reviews\n",
        "ordered_vocabulary = []\n",
        "for review in X_raw:\n",
        "    ordered_vocabulary += review.split()\n",
        "\n",
        "# turn the vocabulary list into a set to remove duplicates\n",
        "ordered_vocabulary = set(ordered_vocabulary)\n",
        "\n",
        "#create vocabulary dictionary that stores the words as keys and their position in the ordered vocabulary as values\n",
        "dictionary_vocab = dict({(k,v) for v, k in enumerate(ordered_vocabulary)})\n",
        "\n",
        "#turn vocabulary set back into a list so it is ordered\n",
        "ordered_vocabulary = list(ordered_vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c54336f5",
      "metadata": {
        "id": "c54336f5"
      },
      "outputs": [],
      "source": [
        "# create binary bag-of-words vectors from the training data\n",
        "X_train = np.zeros(shape=(len(ordered_vocabulary),len(X_raw_train)),dtype=int)\n",
        "\n",
        "for i,review in enumerate(X_raw_train):\n",
        "    for word in review.split():\n",
        "        X_train[dictionary_vocab[word],i] = 1\n",
        "\n",
        "#add pseudo input\n",
        "X_train = np.insert(X_train,0,1,axis=0)\n",
        "\n",
        "# create binary bag-of-words vectors from the test data\n",
        "X_test = np.zeros(shape=(len(ordered_vocabulary),len(X_raw_test)),dtype=int)\n",
        "\n",
        "for i,review in enumerate(X_raw_test):\n",
        "    for word in review.split():\n",
        "        X_test[dictionary_vocab[word],i] = 1\n",
        "        \n",
        "#add pseudo input\n",
        "X_test = np.insert(X_test,0,1,axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4cd61c4",
      "metadata": {
        "id": "a4cd61c4"
      },
      "source": [
        "# Part 3: Learning framework\n",
        "I decided to update the learning rate once per iteration using a time-based decay formula (source: https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1). I also decided to use a fixed seed for initialising the weights, to ensure reproducibility. Since the loss decreases and the model's training set accuracy is at about 92.125% we can assume that it is learning. \n",
        "\n",
        "This part of the assignment definitely took the most time, but it was also the most educational part seeing as one had to go through the underlying mathematical formulas and really understand them to be able to implement them. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e7d5c8a",
      "metadata": {
        "scrolled": false,
        "id": "4e7d5c8a",
        "outputId": "1e586828-7def-4bb9-d3a7-484a46120677"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fitting...\n",
            "Epoch: 1, current loss: 1600.0128962735917\n",
            "Epoch: 2, current loss: 1600.0326951584263\n",
            "Epoch: 3, current loss: 1600.0150648357765\n",
            "Epoch: 4, current loss: 1600.0337263073502\n",
            "Epoch: 5, current loss: 1462.0186300438806\n",
            "Epoch: 6, current loss: 1600.0325857488779\n",
            "Epoch: 7, current loss: 1422.022415229327\n",
            "Epoch: 8, current loss: 1600.0316737089618\n",
            "Epoch: 9, current loss: 1468.025859331138\n",
            "Epoch: 10, current loss: 1600.0320864820442\n",
            "Epoch: 11, current loss: 1236.0288068972748\n",
            "Epoch: 12, current loss: 1600.0314071503365\n",
            "Epoch: 13, current loss: 1550.0312230307252\n",
            "Epoch: 14, current loss: 1600.0330004051782\n",
            "Epoch: 15, current loss: 1256.0330781424245\n",
            "Epoch: 16, current loss: 1406.0336641903145\n",
            "Epoch: 17, current loss: 1134.0341168683626\n",
            "Epoch: 18, current loss: 618.034266845737\n",
            "Epoch: 19, current loss: 334.03425549901357\n",
            "Epoch: 20, current loss: 276.03422416989497\n",
            "Epoch: 21, current loss: 274.03419902336293\n",
            "Epoch: 22, current loss: 268.03417883888727\n",
            "Epoch: 23, current loss: 266.0341626782296\n",
            "Epoch: 24, current loss: 264.03414979327624\n",
            "Epoch: 25, current loss: 260.0341395641952\n",
            "Epoch: 26, current loss: 260.0341314829539\n",
            "Epoch: 27, current loss: 258.03412513412945\n",
            "Epoch: 28, current loss: 256.0341201748552\n",
            "Epoch: 29, current loss: 252.03411632468698\n",
            "Epoch: 30, current loss: 252.03411335414032\n",
            "Epoch: 31, current loss: 252.03411107753024\n",
            "Epoch: 32, current loss: 252.0341093445922\n",
            "Epoch: 33, current loss: 252.03410803460378\n",
            "accuracy: 92.125\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import math\n",
        "from numpy import ndarray\n",
        "from numpy.linalg import norm\n",
        "\n",
        "reguliser_dampening = 0.01   # lambda\n",
        "learning_rate = 0.001        # gamma\n",
        "decay = 0.01\n",
        "\n",
        "class model:\n",
        "    def __init__(self, learning_rate, reguliser_dampening, weights,decay,max_iterations=20):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.reguliser_dampening = reguliser_dampening\n",
        "        self.weights = weights\n",
        "        self.max_iterations = max_iterations\n",
        "        self.parameter_vector = []\n",
        "        self.decay = decay\n",
        "\n",
        "    def calculate_loss(self,y,X):\n",
        "        regularization = (self.reguliser_dampening/2) * (norm(self.weights))\n",
        "        sum_max = 0\n",
        "        for i in range(X.shape[1]):\n",
        "            sum_max += max(0,(1-y[i]*self.parameter_vector[i]))\n",
        "        loss = regularization + sum_max\n",
        "        return loss\n",
        "\n",
        "    def gradient_cal(self,y,X):\n",
        "        nabla = self.reguliser_dampening * self.weights\n",
        "        for i in range(len(y)):\n",
        "            if (y[i] * self.parameter_vector[i]) >= 1:\n",
        "                gradient = 0\n",
        "            else:\n",
        "                gradient = np.multiply(-y[i],X[:,i])\n",
        "            nabla += gradient\n",
        "\n",
        "        self.weights = self.weights - self.learning_rate * nabla\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        print(\"fitting...\")\n",
        "    \n",
        "        self.parameter_vector = self.predict(X)   \n",
        "  \n",
        "        # calculate loss\n",
        "        loss = self.calculate_loss(y,X)\n",
        "        best_loss = loss\n",
        "\n",
        "        # calculate new weights\n",
        "        self.gradient_cal(y,X)\n",
        "        \n",
        "        num_iterations = 0\n",
        "        consecutive_best_loss = 0 #counts how many times in a row the loss is higher than the best loss - 0.001\n",
        "        while self.max_iterations > num_iterations:\n",
        "            if consecutive_best_loss == 5:\n",
        "                break\n",
        "            num_iterations += 1\n",
        "            \n",
        "            # update learning rate with decay\n",
        "            self.learning_rate = self.learning_rate * 1/(1 + self.decay * num_iterations)\n",
        "            \n",
        "            self.parameter_vector = self.predict(X)\n",
        "                \n",
        "            loss = self.calculate_loss(y,X)\n",
        "            print(f\"Epoch: {num_iterations}, current loss: {loss}\")\n",
        "            \n",
        "            self.gradient_cal(y,X)\n",
        "            \n",
        "            # stop criterion\n",
        "            if loss < best_loss:\n",
        "                if loss > best_loss - 0.001:\n",
        "                    consecutive_best_loss += 1\n",
        "            else:\n",
        "                consecutive_best_loss = 0 #reset to zero\n",
        "            if loss < best_loss:\n",
        "                best_loss = loss\n",
        "                \n",
        "        final_weights = self.weights\n",
        "                \n",
        "        return final_weights\n",
        "\n",
        "    def predict(self,X):\n",
        "        # sign function\n",
        "        prediction_vector = [0 for i in range(len(X[1]))]\n",
        "        for i in range(len(X[1])):\n",
        "            temp = np.matmul(self.weights.transpose(),X[:,i])\n",
        "            if temp > 0:\n",
        "                prediction_vector[i] = 1\n",
        "            elif temp < 0: \n",
        "                prediction_vector[i] = -1\n",
        "            else:\n",
        "                prediction_vector[i] = 0\n",
        "        \n",
        "        return prediction_vector   \n",
        "\n",
        "    def score(self, X, y):\n",
        "        prediction_vector = self.predict(X) # Put elements in X through the sign function\n",
        "        equal = np.zeros((len(y),1),dtype=\"int\") # np array that stores how many of the model's predictions for y are correct\n",
        "        for i,element in enumerate(y):\n",
        "            if element == prediction_vector[i]:\n",
        "                equal[i] = 1\n",
        "        sum_equal = np.sum(equal) #summing up the result of the comparison between model's predictions and actual labels\n",
        "        sum_equal_len = sum_equal/len(y) #dividing result by length\n",
        "        score = 100*sum_equal_len # multiplying by 100 to get a percentage\n",
        "                \n",
        "        return score\n",
        "    \n",
        "\n",
        "# initialising the weights, seed 75 has shown to be a good first guess\n",
        "np.random.seed(75)\n",
        "weights_array = 0.01*np.random.randn(len(ordered_vocabulary)+1)\n",
        "\n",
        "# create an instance of the class model\n",
        "first_model = model(learning_rate, reguliser_dampening, weights_array,decay,100)\n",
        "\n",
        "# fit the model\n",
        "fitted_weights = first_model.fit(X_train,y_train)\n",
        "\n",
        "# get the training set accuracy\n",
        "score = first_model.score(X_train,y_train)\n",
        "print(f\"accuracy: {score}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c55d3dc9",
      "metadata": {
        "id": "c55d3dc9",
        "outputId": "780b2f0c-787d-455e-d6d1-91e4feac1720"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        " \n",
        "plt.figure(figsize=(20, 3))\n",
        "plt.plot(first_model.weights[1:])\n",
        "plt.xlabel(\"Value\")\n",
        "plt.xlabel(\"Weights\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1cd0450",
      "metadata": {
        "id": "e1cd0450",
        "outputId": "bbd28012-1a58-49cd-a49c-12bb53373f45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                Word   Weight  Occurences\n",
            "            supposed   -0.475\t321 \n",
            "                very   0.477\t1552 \n",
            "                 why   -0.485\t586 \n",
            "                many   0.489\t796 \n",
            "              should   -0.492\t708 \n",
            "             nothing   -0.492\t594 \n",
            "                also   0.498\t1071 \n",
            "               world   0.505\t643 \n",
            "              stupid   -0.515\t227 \n",
            "                   !   -0.516\t663 \n",
            "                best   0.520\t867 \n",
            "               great   0.529\t805 \n",
            "                   ?   -0.561\t1228 \n",
            "              boring   -0.565\t229 \n",
            "                plot   -0.572\t1002 \n",
            "                both   0.574\t683 \n",
            "                life   0.602\t895 \n",
            "              script   -0.612\t632 \n",
            "               worst   -0.671\t238 \n",
            "                 bad   -1.056\t825 \n"
          ]
        }
      ],
      "source": [
        "assert (len(first_model.weights)-1) == len(ordered_vocabulary)\n",
        "\n",
        "# Sort by absolute value\n",
        "idx = np.argsort(np.abs(first_model.weights[1:]))\n",
        "\n",
        "print(\"                Word   Weight  Occurences\")\n",
        "for i in idx[-20:]:   # Pick those with highest 'voting' values\n",
        "  print(\"%20s   %.3f\\t%i \" % (ordered_vocabulary[i], first_model.weights[i+1], np.sum([ordered_vocabulary[i] in d for d in X_raw])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "303d1c8d",
      "metadata": {
        "id": "303d1c8d"
      },
      "source": [
        "# Part 4: Exploring hyperparameters\n",
        "The highest training set accuracy we get by exploring the hyperparameters in this way is 95.8125%. Therefore I decided to use the learning rate, reguliser dampening and decay used to get this training accuracy also to get a test set accuracy score below. The test set accuracy is 85.5% which is not as good as the original sklearn implementation but still clearly over 50%, indicating that the model was able to generalise from the training data to the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12330efd",
      "metadata": {
        "id": "12330efd",
        "outputId": "a7255ad5-b8c9-40b3-809d-42a2b45b4213"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "learning rate:\treguliser dampening:\tdecay:\ttraining accuracy:\n",
            "fitting...\n",
            "Epoch: 1, current loss: 1606.276521353997\n",
            "Epoch: 2, current loss: 1628.7339383590477\n",
            "Epoch: 3, current loss: 1609.6312598347245\n",
            "Epoch: 4, current loss: 1629.522976287507\n",
            "Epoch: 5, current loss: 1559.8830520500344\n",
            "Epoch: 6, current loss: 1629.5750437544423\n",
            "Epoch: 7, current loss: 1183.9415221798013\n",
            "Epoch: 8, current loss: 1624.847868861924\n",
            "Epoch: 9, current loss: 1621.7616608743688\n",
            "Epoch: 10, current loss: 1626.6546049838157\n",
            "Epoch: 11, current loss: 1588.3124728711848\n",
            "Epoch: 12, current loss: 1628.03529550657\n",
            "Epoch: 13, current loss: 1258.5288122834534\n",
            "Epoch: 14, current loss: 1627.86581261418\n",
            "Epoch: 15, current loss: 1504.2235759546293\n",
            "Epoch: 16, current loss: 1554.924259640384\n",
            "Epoch: 17, current loss: 1253.3224060387101\n",
            "Epoch: 18, current loss: 801.470814871794\n",
            "Epoch: 19, current loss: 419.4298874797524\n",
            "Epoch: 20, current loss: 315.347135239857\n",
            "Epoch: 21, current loss: 313.279692529539\n",
            "Epoch: 22, current loss: 303.22450976253594\n",
            "Epoch: 23, current loss: 305.17960213705504\n",
            "Epoch: 24, current loss: 295.1432669531737\n",
            "Epoch: 25, current loss: 293.11404704699106\n",
            "Epoch: 26, current loss: 291.09069815178646\n",
            "Epoch: 27, current loss: 291.07216474019043\n",
            "Epoch: 28, current loss: 291.0575537455177\n",
            "Epoch: 29, current loss: 291.04611453568293\n",
            "Epoch: 30, current loss: 291.0372211272013\n",
            "Epoch: 31, current loss: 291.03035567608066\n",
            "Epoch: 32, current loss: 291.02509333386945\n",
            "Epoch: 33, current loss: 291.0210884871624\n",
            "Epoch: 34, current loss: 291.01806238653046\n",
            "Epoch: 35, current loss: 291.01579215219147\n",
            "Epoch: 36, current loss: 291.01410112126314\n",
            "Epoch: 37, current loss: 291.0128504782833\n",
            "Epoch: 38, current loss: 291.01193208967277\n",
            "Epoch: 39, current loss: 289.01126244600323\n",
            "Epoch: 40, current loss: 289.0107776041458\n",
            "Epoch: 41, current loss: 289.01042901708837\n",
            "Epoch: 42, current loss: 289.0101801345552\n",
            "Epoch: 43, current loss: 289.0100036659224\n",
            "0.009768585201429125\t0.9542525683758001\t0.009768585201429125\t91.875\n",
            "fitting...\n",
            "Epoch: 1, current loss: 1601.9787789458067\n",
            "Epoch: 2, current loss: 1608.2938848999866\n",
            "Epoch: 3, current loss: 374.48329731290323\n",
            "Epoch: 4, current loss: 536.4444988776534\n",
            "Epoch: 5, current loss: 1602.7097080535655\n",
            "Epoch: 6, current loss: 1603.3388784329222\n",
            "Epoch: 7, current loss: 240.86867209232048\n",
            "Epoch: 8, current loss: 234.86363858662304\n",
            "Epoch: 9, current loss: 222.86096067553808\n",
            "Epoch: 10, current loss: 218.85957299998046\n",
            "Epoch: 11, current loss: 212.85887884820818\n",
            "Epoch: 12, current loss: 212.85854496720867\n",
            "Epoch: 13, current loss: 212.85839090145515\n",
            "Epoch: 14, current loss: 212.85832270736844\n",
            "Epoch: 15, current loss: 212.85829373242262\n",
            "Epoch: 16, current loss: 212.85828190133535\n",
            "0.030710690833316446\t0.09654893846056296\t0.09654893846056296\t93.4375\n",
            "fitting...\n",
            "Epoch: 1, current loss: 1600.2002071842683\n",
            "Epoch: 2, current loss: 1600.68368553202\n",
            "Epoch: 3, current loss: 1600.2621128069068\n",
            "Epoch: 4, current loss: 1600.26249414353\n",
            "Epoch: 5, current loss: 434.24585329092906\n",
            "Epoch: 6, current loss: 366.24548423445253\n",
            "Epoch: 7, current loss: 366.2453524384593\n",
            "Epoch: 8, current loss: 366.2453106602595\n",
            "Epoch: 9, current loss: 366.2452985100468\n",
            "Epoch: 10, current loss: 366.245295256961\n",
            "Epoch: 11, current loss: 366.2452944509908\n",
            "0.09654893846056296\t0.00310723250595386\t0.30353265475060387\t88.5625\n",
            "fitting...\n",
            "Epoch: 1, current loss: 1600.0007310346189\n",
            "Epoch: 2, current loss: 1600.0013879441913\n",
            "Epoch: 3, current loss: 1600.0009024861542\n",
            "Epoch: 4, current loss: 1600.000805951487\n",
            "Epoch: 5, current loss: 1600.0007884526615\n",
            "Epoch: 6, current loss: 1600.0007855219628\n",
            "Epoch: 7, current loss: 1600.0007850888283\n",
            "Epoch: 8, current loss: 1600.0007850324791\n",
            "Epoch: 9, current loss: 1600.0007850259535\n",
            "Epoch: 10, current loss: 1600.000785025273\n",
            "Epoch: 11, current loss: 1600.0007850252084\n",
            "Epoch: 12, current loss: 1600.0007850252027\n",
            "Epoch: 13, current loss: 1600.0007850252023\n",
            "Epoch: 14, current loss: 1600.0007850252023\n",
            "Epoch: 15, current loss: 1600.0007850252023\n",
            "Epoch: 16, current loss: 1600.0007850252023\n",
            "Epoch: 17, current loss: 1600.0007850252023\n",
            "Epoch: 18, current loss: 1600.0007850252023\n",
            "Epoch: 19, current loss: 1600.0007850252023\n",
            "Epoch: 20, current loss: 1600.0007850252023\n",
            "Epoch: 21, current loss: 1600.0007850252023\n",
            "Epoch: 22, current loss: 1600.0007850252023\n",
            "Epoch: 23, current loss: 1600.0007850252023\n",
            "Epoch: 24, current loss: 1600.0007850252023\n",
            "Epoch: 25, current loss: 1600.0007850252023\n",
            "Epoch: 26, current loss: 1600.0007850252023\n",
            "Epoch: 27, current loss: 1600.0007850252023\n",
            "Epoch: 28, current loss: 1600.0007850252023\n",
            "Epoch: 29, current loss: 1600.0007850252023\n",
            "Epoch: 30, current loss: 1600.0007850252023\n",
            "Epoch: 31, current loss: 1600.0007850252023\n",
            "Epoch: 32, current loss: 1600.0007850252023\n",
            "Epoch: 33, current loss: 1600.0007850252023\n",
            "Epoch: 34, current loss: 1600.0007850252023\n",
            "Epoch: 35, current loss: 1600.0007850252023\n",
            "Epoch: 36, current loss: 1600.0007850252023\n",
            "Epoch: 37, current loss: 1600.0007850252023\n",
            "Epoch: 38, current loss: 1600.0007850252023\n",
            "Epoch: 39, current loss: 1600.0007850252023\n",
            "Epoch: 40, current loss: 1600.0007850252023\n",
            "Epoch: 41, current loss: 1600.0007850252023\n",
            "Epoch: 42, current loss: 1600.0007850252023\n",
            "Epoch: 43, current loss: 1600.0007850252023\n",
            "Epoch: 44, current loss: 1600.0007850252023\n",
            "Epoch: 45, current loss: 1600.0007850252023\n",
            "Epoch: 46, current loss: 1600.0007850252023\n",
            "Epoch: 47, current loss: 1600.0007850252023\n",
            "Epoch: 48, current loss: 1600.0007850252023\n",
            "Epoch: 49, current loss: 1600.0007850252023\n",
            "Epoch: 50, current loss: 1600.0007850252023\n",
            "Epoch: 51, current loss: 1600.0007850252023\n",
            "Epoch: 52, current loss: 1600.0007850252023\n",
            "Epoch: 53, current loss: 1600.0007850252023\n",
            "Epoch: 54, current loss: 1600.0007850252023\n",
            "Epoch: 55, current loss: 1600.0007850252023\n",
            "Epoch: 56, current loss: 1600.0007850252023\n",
            "Epoch: 57, current loss: 1600.0007850252023\n",
            "Epoch: 58, current loss: 1600.0007850252023\n",
            "Epoch: 59, current loss: 1600.0007850252023\n",
            "Epoch: 60, current loss: 1600.0007850252023\n",
            "Epoch: 61, current loss: 1600.0007850252023\n",
            "Epoch: 62, current loss: 1600.0007850252023\n",
            "Epoch: 63, current loss: 1600.0007850252023\n",
            "Epoch: 64, current loss: 1600.0007850252023\n",
            "Epoch: 65, current loss: 1600.0007850252023\n",
            "Epoch: 66, current loss: 1600.0007850252023\n",
            "Epoch: 67, current loss: 1600.0007850252023\n",
            "Epoch: 68, current loss: 1600.0007850252023\n",
            "Epoch: 69, current loss: 1600.0007850252023\n",
            "Epoch: 70, current loss: 1600.0007850252023\n",
            "Epoch: 71, current loss: 1600.0007850252023\n",
            "Epoch: 72, current loss: 1600.0007850252023\n",
            "Epoch: 73, current loss: 1600.0007850252023\n",
            "Epoch: 74, current loss: 1600.0007850252023\n",
            "Epoch: 75, current loss: 1600.0007850252023\n",
            "Epoch: 76, current loss: 1600.0007850252023\n",
            "Epoch: 77, current loss: 1600.0007850252023\n",
            "Epoch: 78, current loss: 1600.0007850252023\n",
            "Epoch: 79, current loss: 1600.0007850252023\n",
            "Epoch: 80, current loss: 1600.0007850252023\n",
            "Epoch: 81, current loss: 1600.0007850252023\n",
            "Epoch: 82, current loss: 1600.0007850252023\n",
            "Epoch: 83, current loss: 1600.0007850252023\n",
            "Epoch: 84, current loss: 1600.0007850252023\n",
            "Epoch: 85, current loss: 1600.0007850252023\n",
            "Epoch: 86, current loss: 1600.0007850252023\n",
            "Epoch: 87, current loss: 1600.0007850252023\n",
            "Epoch: 88, current loss: 1600.0007850252023\n",
            "Epoch: 89, current loss: 1600.0007850252023\n",
            "Epoch: 90, current loss: 1600.0007850252023\n",
            "Epoch: 91, current loss: 1600.0007850252023\n",
            "Epoch: 92, current loss: 1600.0007850252023\n",
            "Epoch: 93, current loss: 1600.0007850252023\n",
            "Epoch: 94, current loss: 1600.0007850252023\n",
            "Epoch: 95, current loss: 1600.0007850252023\n",
            "Epoch: 96, current loss: 1600.0007850252023\n",
            "Epoch: 97, current loss: 1600.0007850252023\n",
            "Epoch: 98, current loss: 1600.0007850252023\n",
            "Epoch: 99, current loss: 1600.0007850252023\n",
            "Epoch: 100, current loss: 1600.0007850252023\n",
            "0.00310723250595386\t0.0003143821771531824\t0.9542525683758001\t50.0\n",
            "fitting...\n",
            "Epoch: 1, current loss: 1603.3807302225773\n",
            "Epoch: 2, current loss: 1603.5017102258075\n",
            "Epoch: 3, current loss: 1603.3875010002128\n",
            "Epoch: 4, current loss: 1603.5160679016085\n",
            "Epoch: 5, current loss: 1579.4037777301642\n",
            "Epoch: 6, current loss: 1603.5339385590632\n",
            "Epoch: 7, current loss: 1461.4288567865424\n",
            "Epoch: 8, current loss: 1603.5365273331963\n",
            "Epoch: 9, current loss: 1585.4616979494522\n",
            "Epoch: 10, current loss: 1603.570161291643\n",
            "Epoch: 11, current loss: 1491.501220306369\n",
            "Epoch: 12, current loss: 1603.5935571687899\n",
            "Epoch: 13, current loss: 1533.5466598521919\n",
            "Epoch: 14, current loss: 1603.6305614277328\n",
            "Epoch: 15, current loss: 1513.59708027464\n",
            "Epoch: 16, current loss: 1603.6698296999684\n",
            "Epoch: 17, current loss: 1513.6515562204208\n",
            "Epoch: 18, current loss: 1603.7138112229572\n",
            "Epoch: 19, current loss: 1511.709142112197\n",
            "Epoch: 20, current loss: 1603.7611920019685\n",
            "Epoch: 21, current loss: 1511.7689153542435\n",
            "Epoch: 22, current loss: 1603.8119650238718\n",
            "Epoch: 23, current loss: 1493.8300147833743\n",
            "Epoch: 24, current loss: 1581.8629491286424\n",
            "Epoch: 25, current loss: 1487.8896752011017\n",
            "Epoch: 26, current loss: 1509.9150103903717\n",
            "Epoch: 27, current loss: 1417.9424181446882\n",
            "Epoch: 28, current loss: 1341.9609570713442\n",
            "Epoch: 29, current loss: 1279.9818745272792\n",
            "Epoch: 30, current loss: 1129.996284415292\n",
            "Epoch: 31, current loss: 966.008171169263\n",
            "Epoch: 32, current loss: 694.0134260268079\n",
            "Epoch: 33, current loss: 470.01451295160285\n",
            "Epoch: 34, current loss: 352.01263491193043\n",
            "Epoch: 35, current loss: 302.01026323661944\n",
            "Epoch: 36, current loss: 278.0079637011477\n",
            "Epoch: 37, current loss: 274.005825674223\n",
            "Epoch: 38, current loss: 266.0038411207398\n",
            "Epoch: 39, current loss: 254.0019995894971\n",
            "Epoch: 40, current loss: 248.00028812572444\n",
            "Epoch: 41, current loss: 239.99869640287153\n",
            "Epoch: 42, current loss: 229.99721378393374\n",
            "Epoch: 43, current loss: 227.99583165280748\n",
            "Epoch: 44, current loss: 219.99454367916002\n",
            "Epoch: 45, current loss: 213.99334257254043\n",
            "Epoch: 46, current loss: 207.9922219964813\n",
            "Epoch: 47, current loss: 207.99117590459977\n",
            "Epoch: 48, current loss: 197.99019936336808\n",
            "Epoch: 49, current loss: 197.9892863569182\n",
            "Epoch: 50, current loss: 201.98843341325804\n",
            "Epoch: 51, current loss: 193.98763690422987\n",
            "Epoch: 52, current loss: 191.98689293495167\n",
            "Epoch: 53, current loss: 189.98619769903055\n",
            "Epoch: 54, current loss: 189.98554796960667\n",
            "Epoch: 55, current loss: 185.98494075734314\n",
            "Epoch: 56, current loss: 181.9843733464558\n",
            "Epoch: 57, current loss: 183.98384316408695\n",
            "Epoch: 58, current loss: 173.98334792476754\n",
            "Epoch: 59, current loss: 167.98288495579607\n",
            "Epoch: 60, current loss: 169.9824523161833\n",
            "Epoch: 61, current loss: 165.9820481859809\n",
            "Epoch: 62, current loss: 165.981670806128\n",
            "Epoch: 63, current loss: 161.98131857699744\n",
            "Epoch: 64, current loss: 159.9809897998252\n",
            "Epoch: 65, current loss: 155.98068300967512\n",
            "Epoch: 66, current loss: 153.98039680524172\n",
            "Epoch: 67, current loss: 155.98012993584385\n",
            "Epoch: 68, current loss: 153.97988123948696\n",
            "Epoch: 69, current loss: 153.97964959791824\n",
            "Epoch: 70, current loss: 149.97943395050083\n",
            "Epoch: 71, current loss: 149.9792332923259\n",
            "Epoch: 72, current loss: 145.97904666264932\n",
            "Epoch: 73, current loss: 145.97887318518573\n",
            "Epoch: 74, current loss: 143.97871202998363\n",
            "Epoch: 75, current loss: 143.97856242228454\n",
            "0.00010000000000000009\t3.0000000000000004\t0.0009883615331157487\t95.8125\n",
            "fitting...\n",
            "Epoch: 1, current loss: 1600.3458688911765\n",
            "Epoch: 2, current loss: 1600.4495230942805\n",
            "Epoch: 3, current loss: 1600.3536567115016\n",
            "Epoch: 4, current loss: 1600.4564277941984\n",
            "Epoch: 5, current loss: 1448.3687328628503\n",
            "Epoch: 6, current loss: 1600.4471184446725\n",
            "Epoch: 7, current loss: 1492.3874055857766\n",
            "Epoch: 8, current loss: 1600.44757651523\n",
            "Epoch: 9, current loss: 1248.4066436171777\n",
            "Epoch: 10, current loss: 1600.4355476042215\n",
            "Epoch: 11, current loss: 1588.4248827747288\n",
            "Epoch: 12, current loss: 1600.4477254082647\n",
            "Epoch: 13, current loss: 1276.4401836920167\n",
            "Epoch: 14, current loss: 1600.449842984262\n",
            "Epoch: 15, current loss: 1432.4526420828793\n",
            "Epoch: 16, current loss: 1492.4575549943518\n",
            "Epoch: 17, current loss: 1176.460827580951\n",
            "Epoch: 18, current loss: 666.4620268053504\n",
            "Epoch: 19, current loss: 362.4619676199083\n",
            "Epoch: 20, current loss: 288.46172675767997\n",
            "Epoch: 21, current loss: 288.4615337983442\n",
            "Epoch: 22, current loss: 286.4613784118476\n",
            "Epoch: 23, current loss: 278.4612536933242\n",
            "Epoch: 24, current loss: 274.4611539058859\n",
            "Epoch: 25, current loss: 270.46107439344536\n",
            "Epoch: 26, current loss: 264.4610113270391\n",
            "Epoch: 27, current loss: 258.4609615521509\n",
            "Epoch: 28, current loss: 256.46092247882757\n",
            "Epoch: 29, current loss: 254.46089199216982\n",
            "Epoch: 30, current loss: 254.46086835227524\n",
            "Epoch: 31, current loss: 254.46085014047736\n",
            "Epoch: 32, current loss: 254.46083620343086\n",
            "Epoch: 33, current loss: 254.46082560976814\n",
            "0.0003143821771531824\t0.30353265475060387\t0.009768585201429125\t92.0625\n",
            "fitting...\n",
            "Epoch: 1, current loss: 1661.4803338772567\n",
            "Epoch: 2, current loss: 1890.6118570983544\n",
            "Epoch: 3, current loss: 1704.1369968742551\n",
            "Epoch: 4, current loss: 1886.3746077529008\n",
            "Epoch: 5, current loss: 1743.3830886255976\n",
            "Epoch: 6, current loss: 1888.31311751986\n",
            "Epoch: 7, current loss: 1776.4793504511624\n",
            "Epoch: 8, current loss: 1893.4351884119476\n",
            "Epoch: 9, current loss: 1803.8566949379942\n",
            "Epoch: 10, current loss: 1899.8243974841319\n",
            "Epoch: 11, current loss: 1826.3248066072845\n",
            "Epoch: 12, current loss: 1906.3653767568574\n",
            "Epoch: 13, current loss: 1844.674052580374\n",
            "Epoch: 14, current loss: 1912.4650965337278\n",
            "Epoch: 15, current loss: 1859.5991648562672\n",
            "Epoch: 16, current loss: 1917.8460576274013\n",
            "Epoch: 17, current loss: 1871.690415549962\n",
            "Epoch: 18, current loss: 1922.4116803155318\n",
            "Epoch: 19, current loss: 1881.4420730557385\n",
            "Epoch: 20, current loss: 1926.1641059829424\n",
            "Epoch: 21, current loss: 1889.2650026199115\n",
            "Epoch: 22, current loss: 1929.1557595859908\n",
            "Epoch: 23, current loss: 1895.4993891526847\n",
            "Epoch: 24, current loss: 1931.4615406360324\n",
            "Epoch: 25, current loss: 1900.4262969050287\n",
            "Epoch: 26, current loss: 1933.1633021327755\n",
            "Epoch: 27, current loss: 1904.2777193576803\n",
            "Epoch: 28, current loss: 1934.3415417040358\n",
            "Epoch: 29, current loss: 1907.245119633972\n",
            "Epoch: 30, current loss: 1935.071271173856\n",
            "Epoch: 31, current loss: 1909.4865901442536\n",
            "Epoch: 32, current loss: 1935.420265376268\n",
            "Epoch: 33, current loss: 1911.132800903145\n",
            "Epoch: 34, current loss: 1935.448628373958\n",
            "Epoch: 35, current loss: 1912.291909836034\n",
            "Epoch: 36, current loss: 1935.20905429526\n",
            "Epoch: 37, current loss: 1913.0535967934325\n",
            "Epoch: 38, current loss: 1934.7474215112975\n",
            "Epoch: 39, current loss: 1913.4923654957256\n",
            "Epoch: 40, current loss: 1934.1035146529948\n",
            "Epoch: 41, current loss: 1913.670238691478\n",
            "Epoch: 42, current loss: 1933.3117615489577\n",
            "Epoch: 43, current loss: 1913.638953516024\n",
            "Epoch: 44, current loss: 1932.4019268185866\n",
            "Epoch: 45, current loss: 1913.44174731474\n",
            "Epoch: 46, current loss: 1931.3997356831455\n",
            "Epoch: 47, current loss: 1913.1148093943843\n",
            "Epoch: 48, current loss: 1930.327419647922\n",
            "Epoch: 49, current loss: 1912.6884613341701\n",
            "Epoch: 50, current loss: 1929.2041855602693\n",
            "Epoch: 51, current loss: 1908.1881175232393\n",
            "Epoch: 52, current loss: 1927.6260797343543\n",
            "Epoch: 53, current loss: 1905.6269329469712\n",
            "Epoch: 54, current loss: 1925.898167723993\n",
            "Epoch: 55, current loss: 1905.0188088088098\n",
            "Epoch: 56, current loss: 1924.260720136137\n",
            "Epoch: 57, current loss: 1902.3790012140248\n",
            "Epoch: 58, current loss: 1922.51059948526\n",
            "Epoch: 59, current loss: 1899.7188851003443\n",
            "Epoch: 60, current loss: 1920.7159339082818\n",
            "Epoch: 61, current loss: 1889.0467110371762\n",
            "Epoch: 62, current loss: 1918.4386085727738\n",
            "Epoch: 63, current loss: 1884.3609646547043\n",
            "Epoch: 64, current loss: 1916.1149114619159\n",
            "Epoch: 65, current loss: 1881.6683375742284\n",
            "Epoch: 66, current loss: 1913.9016062026767\n",
            "Epoch: 67, current loss: 1874.9746093372005\n",
            "Epoch: 68, current loss: 1911.6248406991976\n",
            "Epoch: 69, current loss: 1874.2836656710767\n",
            "Epoch: 70, current loss: 1909.6178604286652\n",
            "Epoch: 71, current loss: 1871.5992974528267\n",
            "Epoch: 72, current loss: 1907.7538118819143\n",
            "Epoch: 73, current loss: 1866.92495435069\n",
            "Epoch: 74, current loss: 1905.9742165929792\n",
            "Epoch: 75, current loss: 1860.2633936524985\n",
            "Epoch: 76, current loss: 1894.166660738661\n",
            "Epoch: 77, current loss: 1841.312818132858\n",
            "Epoch: 78, current loss: 1878.045890376311\n",
            "Epoch: 79, current loss: 1833.9873841403007\n",
            "Epoch: 80, current loss: 1845.9895431316845\n",
            "Epoch: 81, current loss: 1815.858374801632\n",
            "Epoch: 82, current loss: 1813.6398172624708\n",
            "Epoch: 83, current loss: 1773.1065994907838\n",
            "Epoch: 84, current loss: 1782.538297893418\n",
            "Epoch: 85, current loss: 1749.9395119200265\n",
            "Epoch: 86, current loss: 1719.3146125011103\n",
            "Epoch: 87, current loss: 1677.924818990568\n",
            "Epoch: 88, current loss: 1647.0538757701975\n",
            "Epoch: 89, current loss: 1599.1120203809073\n",
            "Epoch: 90, current loss: 1545.9211154882855\n",
            "Epoch: 91, current loss: 1491.34128907621\n",
            "Epoch: 92, current loss: 1395.6997055491352\n",
            "Epoch: 93, current loss: 1322.3401385978907\n",
            "Epoch: 94, current loss: 1172.1281004796028\n",
            "Epoch: 95, current loss: 1047.7633773817872\n",
            "Epoch: 96, current loss: 850.6925896007157\n",
            "Epoch: 97, current loss: 651.3830985613665\n",
            "Epoch: 98, current loss: 555.8763461260148\n",
            "Epoch: 99, current loss: 502.5202135027803\n",
            "Epoch: 100, current loss: 473.4273189182104\n",
            "0.030710690833316446\t3.0000000000000004\t0.0003143821771531824\t92.6875\n",
            "fitting...\n",
            "Epoch: 1, current loss: 1600.022714798779\n",
            "Epoch: 2, current loss: 1600.0939680898562\n",
            "Epoch: 3, current loss: 1600.0328310659884\n",
            "Epoch: 4, current loss: 1600.0973450845895\n",
            "Epoch: 5, current loss: 1478.0467944304103\n",
            "Epoch: 6, current loss: 1600.0947972219717\n",
            "Epoch: 7, current loss: 1364.0602937524845\n",
            "Epoch: 8, current loss: 1600.0893111543596\n",
            "Epoch: 9, current loss: 1560.0720942022567\n",
            "Epoch: 10, current loss: 1600.0938629994075\n",
            "Epoch: 11, current loss: 1090.082004719854\n",
            "Epoch: 12, current loss: 1600.088704485505\n",
            "Epoch: 13, current loss: 1594.0903834129915\n",
            "Epoch: 14, current loss: 1600.0948518734535\n",
            "Epoch: 15, current loss: 1450.096271265231\n",
            "Epoch: 16, current loss: 1496.0987178537496\n",
            "Epoch: 17, current loss: 1160.100192144648\n",
            "Epoch: 18, current loss: 672.1007586545506\n",
            "Epoch: 19, current loss: 364.10073883393204\n",
            "Epoch: 20, current loss: 280.10063021719316\n",
            "Epoch: 21, current loss: 280.1005428202273\n",
            "Epoch: 22, current loss: 272.1004723815036\n",
            "Epoch: 23, current loss: 264.1004157823051\n",
            "Epoch: 24, current loss: 264.10037045230985\n",
            "Epoch: 25, current loss: 264.10033431041535\n",
            "Epoch: 26, current loss: 262.100305638533\n",
            "Epoch: 27, current loss: 262.1002830118323\n",
            "Epoch: 28, current loss: 262.10026525712095\n",
            "0.00310723250595386\t0.009768585201429125\t0.009768585201429125\t91.8125\n",
            "fitting...\n",
            "Epoch: 1, current loss: 1600.6294196168267\n",
            "Epoch: 2, current loss: 1601.301035679054\n",
            "Epoch: 3, current loss: 1600.8100608619689\n",
            "Epoch: 4, current loss: 1600.7089120630444\n",
            "Epoch: 5, current loss: 1600.6903413355967\n",
            "Epoch: 6, current loss: 1600.687222818532\n",
            "Epoch: 7, current loss: 1600.6867617178175\n",
            "Epoch: 8, current loss: 1600.6867017263276\n",
            "Epoch: 9, current loss: 1600.6866947787967\n",
            "Epoch: 10, current loss: 1600.6866940542193\n",
            "Epoch: 11, current loss: 1600.6866939854904\n",
            "Epoch: 12, current loss: 1600.6866939795123\n",
            "Epoch: 13, current loss: 1600.686693979032\n",
            "Epoch: 14, current loss: 1600.6866939789963\n",
            "Epoch: 15, current loss: 1600.6866939789938\n",
            "Epoch: 16, current loss: 1600.6866939789936\n",
            "Epoch: 17, current loss: 1600.6866939789936\n",
            "Epoch: 18, current loss: 1600.6866939789936\n",
            "Epoch: 19, current loss: 1600.6866939789936\n",
            "Epoch: 20, current loss: 1600.6866939789936\n",
            "Epoch: 21, current loss: 1600.6866939789936\n",
            "Epoch: 22, current loss: 1600.6866939789936\n",
            "Epoch: 23, current loss: 1600.6866939789936\n",
            "Epoch: 24, current loss: 1600.6866939789936\n",
            "Epoch: 25, current loss: 1600.6866939789936\n",
            "Epoch: 26, current loss: 1600.6866939789936\n",
            "Epoch: 27, current loss: 1600.6866939789936\n",
            "Epoch: 28, current loss: 1600.6866939789936\n",
            "Epoch: 29, current loss: 1600.6866939789936\n",
            "Epoch: 30, current loss: 1600.6866939789936\n",
            "Epoch: 31, current loss: 1600.6866939789936\n",
            "Epoch: 32, current loss: 1600.6866939789936\n",
            "Epoch: 33, current loss: 1600.6866939789936\n",
            "Epoch: 34, current loss: 1600.6866939789936\n",
            "Epoch: 35, current loss: 1600.6866939789936\n",
            "Epoch: 36, current loss: 1600.6866939789936\n",
            "Epoch: 37, current loss: 1600.6866939789936\n",
            "Epoch: 38, current loss: 1600.6866939789936\n",
            "Epoch: 39, current loss: 1600.6866939789936\n",
            "Epoch: 40, current loss: 1600.6866939789936\n",
            "Epoch: 41, current loss: 1600.6866939789936\n",
            "Epoch: 42, current loss: 1600.6866939789936\n",
            "Epoch: 43, current loss: 1600.6866939789936\n",
            "Epoch: 44, current loss: 1600.6866939789936\n",
            "Epoch: 45, current loss: 1600.6866939789936\n",
            "Epoch: 46, current loss: 1600.6866939789936\n",
            "Epoch: 47, current loss: 1600.6866939789936\n",
            "Epoch: 48, current loss: 1600.6866939789936\n",
            "Epoch: 49, current loss: 1600.6866939789936\n",
            "Epoch: 50, current loss: 1600.6866939789936\n",
            "Epoch: 51, current loss: 1600.6866939789936\n",
            "Epoch: 52, current loss: 1600.6866939789936\n",
            "Epoch: 53, current loss: 1600.6866939789936\n",
            "Epoch: 54, current loss: 1600.6866939789936\n",
            "Epoch: 55, current loss: 1600.6866939789936\n",
            "Epoch: 56, current loss: 1600.6866939789936\n",
            "Epoch: 57, current loss: 1600.6866939789936\n",
            "Epoch: 58, current loss: 1600.6866939789936\n",
            "Epoch: 59, current loss: 1600.6866939789936\n",
            "Epoch: 60, current loss: 1600.6866939789936\n",
            "Epoch: 61, current loss: 1600.6866939789936\n",
            "Epoch: 62, current loss: 1600.6866939789936\n",
            "Epoch: 63, current loss: 1600.6866939789936\n",
            "Epoch: 64, current loss: 1600.6866939789936\n",
            "Epoch: 65, current loss: 1600.6866939789936\n",
            "Epoch: 66, current loss: 1600.6866939789936\n",
            "Epoch: 67, current loss: 1600.6866939789936\n",
            "Epoch: 68, current loss: 1600.6866939789936\n",
            "Epoch: 69, current loss: 1600.6866939789936\n",
            "Epoch: 70, current loss: 1600.6866939789936\n",
            "Epoch: 71, current loss: 1600.6866939789936\n",
            "Epoch: 72, current loss: 1600.6866939789936\n",
            "Epoch: 73, current loss: 1600.6866939789936\n",
            "Epoch: 74, current loss: 1600.6866939789936\n",
            "Epoch: 75, current loss: 1600.6866939789936\n",
            "Epoch: 76, current loss: 1600.6866939789936\n",
            "Epoch: 77, current loss: 1600.6866939789936\n",
            "Epoch: 78, current loss: 1600.6866939789936\n",
            "Epoch: 79, current loss: 1600.6866939789936\n",
            "Epoch: 80, current loss: 1600.6866939789936\n",
            "Epoch: 81, current loss: 1600.6866939789936\n",
            "Epoch: 82, current loss: 1600.6866939789936\n",
            "Epoch: 83, current loss: 1600.6866939789936\n",
            "Epoch: 84, current loss: 1600.6866939789936\n",
            "Epoch: 85, current loss: 1600.6866939789936\n",
            "Epoch: 86, current loss: 1600.6866939789936\n",
            "Epoch: 87, current loss: 1600.6866939789936\n",
            "Epoch: 88, current loss: 1600.6866939789936\n",
            "Epoch: 89, current loss: 1600.6866939789936\n",
            "Epoch: 90, current loss: 1600.6866939789936\n",
            "Epoch: 91, current loss: 1600.6866939789936\n",
            "Epoch: 92, current loss: 1600.6866939789936\n",
            "Epoch: 93, current loss: 1600.6866939789936\n",
            "Epoch: 94, current loss: 1600.6866939789936\n",
            "Epoch: 95, current loss: 1600.6866939789936\n",
            "Epoch: 96, current loss: 1600.6866939789936\n",
            "Epoch: 97, current loss: 1600.6866939789936\n",
            "Epoch: 98, current loss: 1600.6866939789936\n",
            "Epoch: 99, current loss: 1600.6866939789936\n",
            "Epoch: 100, current loss: 1600.6866939789936\n",
            "0.030710690833316446\t0.030710690833316446\t0.9542525683758001\t50.0\n",
            "fitting...\n",
            "Epoch: 1, current loss: 1600.0637092070301\n",
            "Epoch: 2, current loss: 1600.1316257116252\n",
            "Epoch: 3, current loss: 1600.0819847164396\n",
            "Epoch: 4, current loss: 1600.071760075855\n",
            "Epoch: 5, current loss: 1600.0698830957208\n",
            "Epoch: 6, current loss: 1600.0695679112705\n",
            "Epoch: 7, current loss: 1600.06952130869\n",
            "Epoch: 8, current loss: 1600.0695152454682\n",
            "Epoch: 9, current loss: 1600.069514543295\n",
            "Epoch: 10, current loss: 1600.0695144700635\n",
            "Epoch: 11, current loss: 1600.069514463117\n",
            "Epoch: 12, current loss: 1600.0695144625129\n",
            "Epoch: 13, current loss: 1600.0695144624644\n",
            "Epoch: 14, current loss: 1600.0695144624608\n",
            "Epoch: 15, current loss: 1600.0695144624606\n",
            "Epoch: 16, current loss: 1600.0695144624606\n",
            "Epoch: 17, current loss: 1600.0695144624606\n",
            "Epoch: 18, current loss: 1600.0695144624606\n",
            "Epoch: 19, current loss: 1600.0695144624606\n",
            "Epoch: 20, current loss: 1600.0695144624606\n",
            "Epoch: 21, current loss: 1600.0695144624606\n",
            "Epoch: 22, current loss: 1600.0695144624606\n",
            "Epoch: 23, current loss: 1600.0695144624606\n",
            "Epoch: 24, current loss: 1600.0695144624606\n",
            "Epoch: 25, current loss: 1600.0695144624606\n",
            "Epoch: 26, current loss: 1600.0695144624606\n",
            "Epoch: 27, current loss: 1600.0695144624606\n",
            "Epoch: 28, current loss: 1600.0695144624606\n",
            "Epoch: 29, current loss: 1600.0695144624606\n",
            "Epoch: 30, current loss: 1600.0695144624606\n",
            "Epoch: 31, current loss: 1600.0695144624606\n",
            "Epoch: 32, current loss: 1600.0695144624606\n",
            "Epoch: 33, current loss: 1600.0695144624606\n",
            "Epoch: 34, current loss: 1600.0695144624606\n",
            "Epoch: 35, current loss: 1600.0695144624606\n",
            "Epoch: 36, current loss: 1600.0695144624606\n",
            "Epoch: 37, current loss: 1600.0695144624606\n",
            "Epoch: 38, current loss: 1600.0695144624606\n",
            "Epoch: 39, current loss: 1600.0695144624606\n",
            "Epoch: 40, current loss: 1600.0695144624606\n",
            "Epoch: 41, current loss: 1600.0695144624606\n",
            "Epoch: 42, current loss: 1600.0695144624606\n",
            "Epoch: 43, current loss: 1600.0695144624606\n",
            "Epoch: 44, current loss: 1600.0695144624606\n",
            "Epoch: 45, current loss: 1600.0695144624606\n",
            "Epoch: 46, current loss: 1600.0695144624606\n",
            "Epoch: 47, current loss: 1600.0695144624606\n",
            "Epoch: 48, current loss: 1600.0695144624606\n",
            "Epoch: 49, current loss: 1600.0695144624606\n",
            "Epoch: 50, current loss: 1600.0695144624606\n",
            "Epoch: 51, current loss: 1600.0695144624606\n",
            "Epoch: 52, current loss: 1600.0695144624606\n",
            "Epoch: 53, current loss: 1600.0695144624606\n",
            "Epoch: 54, current loss: 1600.0695144624606\n",
            "Epoch: 55, current loss: 1600.0695144624606\n",
            "Epoch: 56, current loss: 1600.0695144624606\n",
            "Epoch: 57, current loss: 1600.0695144624606\n",
            "Epoch: 58, current loss: 1600.0695144624606\n",
            "Epoch: 59, current loss: 1600.0695144624606\n",
            "Epoch: 60, current loss: 1600.0695144624606\n",
            "Epoch: 61, current loss: 1600.0695144624606\n",
            "Epoch: 62, current loss: 1600.0695144624606\n",
            "Epoch: 63, current loss: 1600.0695144624606\n",
            "Epoch: 64, current loss: 1600.0695144624606\n",
            "Epoch: 65, current loss: 1600.0695144624606\n",
            "Epoch: 66, current loss: 1600.0695144624606\n",
            "Epoch: 67, current loss: 1600.0695144624606\n",
            "Epoch: 68, current loss: 1600.0695144624606\n",
            "Epoch: 69, current loss: 1600.0695144624606\n",
            "Epoch: 70, current loss: 1600.0695144624606\n",
            "Epoch: 71, current loss: 1600.0695144624606\n",
            "Epoch: 72, current loss: 1600.0695144624606\n",
            "Epoch: 73, current loss: 1600.0695144624606\n",
            "Epoch: 74, current loss: 1600.0695144624606\n",
            "Epoch: 75, current loss: 1600.0695144624606\n",
            "Epoch: 76, current loss: 1600.0695144624606\n",
            "Epoch: 77, current loss: 1600.0695144624606\n",
            "Epoch: 78, current loss: 1600.0695144624606\n",
            "Epoch: 79, current loss: 1600.0695144624606\n",
            "Epoch: 80, current loss: 1600.0695144624606\n",
            "Epoch: 81, current loss: 1600.0695144624606\n",
            "Epoch: 82, current loss: 1600.0695144624606\n",
            "Epoch: 83, current loss: 1600.0695144624606\n",
            "Epoch: 84, current loss: 1600.0695144624606\n",
            "Epoch: 85, current loss: 1600.0695144624606\n",
            "Epoch: 86, current loss: 1600.0695144624606\n",
            "Epoch: 87, current loss: 1600.0695144624606\n",
            "Epoch: 88, current loss: 1600.0695144624606\n",
            "Epoch: 89, current loss: 1600.0695144624606\n",
            "Epoch: 90, current loss: 1600.0695144624606\n",
            "Epoch: 91, current loss: 1600.0695144624606\n",
            "Epoch: 92, current loss: 1600.0695144624606\n",
            "Epoch: 93, current loss: 1600.0695144624606\n",
            "Epoch: 94, current loss: 1600.0695144624606\n",
            "Epoch: 95, current loss: 1600.0695144624606\n",
            "Epoch: 96, current loss: 1600.0695144624606\n",
            "Epoch: 97, current loss: 1600.0695144624606\n",
            "Epoch: 98, current loss: 1600.0695144624606\n",
            "Epoch: 99, current loss: 1600.0695144624606\n",
            "Epoch: 100, current loss: 1600.0695144624606\n",
            "0.9542525683758001\t0.00010000000000000009\t0.9542525683758001\t50.0\n"
          ]
        }
      ],
      "source": [
        "# create list of sample parameters where each element consists of a tuple (learning rate, reguliser dampening, decay)\n",
        "parameters = []\n",
        "\n",
        "learning_rate = np.exp(np.linspace(np.log(0.0001), np.log(3), 10))\n",
        "reguliser_dampening = np.exp(np.linspace(np.log(0.0001), np.log(3), 10))\n",
        "decay = np.exp(np.linspace(np.log(0.0001), np.log(3), 10))\n",
        "               \n",
        "for learn in learning_rate:\n",
        "    for regul in reguliser_dampening:\n",
        "        for dec in decay:\n",
        "               parameters.append((learn,regul,dec))\n",
        "\n",
        "# choose 10 random elements from the list\n",
        "sample_parameters = random.choices(parameters, k=10)\n",
        "\n",
        "print(\"learning rate:\\treguliser dampening:\\tdecay:\\ttraining accuracy:\")\n",
        "\n",
        "#iterate through list and fit and score model once with the parameters in each tuple\n",
        "for sample in sample_parameters:\n",
        "    search_model = model(sample[0],sample[1],weights_array,sample[2],100)\n",
        "    search_model.fit(X_train, y_train)\n",
        "    score = search_model.score(X_train, y_train)\n",
        "    print(f\"{sample[0]}\\t{sample[1]}\\t{sample[2]}\\t{score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab1c388c",
      "metadata": {
        "id": "ab1c388c",
        "outputId": "dce17555-dd0b-4cdc-f84b-7dac17c3043c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fitting...\n",
            "Epoch: 1, current loss: 1603.3807302225773\n",
            "Epoch: 2, current loss: 1603.5017103293535\n",
            "Epoch: 3, current loss: 1603.3875010423783\n",
            "Epoch: 4, current loss: 1603.5160683435017\n",
            "Epoch: 5, current loss: 1579.403777894947\n",
            "Epoch: 6, current loss: 1603.5339396033805\n",
            "Epoch: 7, current loss: 1461.4288572573375\n",
            "Epoch: 8, current loss: 1603.536528988187\n",
            "Epoch: 9, current loss: 1585.4616991936532\n",
            "Epoch: 10, current loss: 1603.5701643450311\n",
            "Epoch: 11, current loss: 1491.5012226657575\n",
            "Epoch: 12, current loss: 1603.5935616438373\n",
            "Epoch: 13, current loss: 1533.546664174559\n",
            "Epoch: 14, current loss: 1603.6305681816875\n",
            "Epoch: 15, current loss: 1513.5970873010538\n",
            "Epoch: 16, current loss: 1603.6698392952403\n",
            "Epoch: 17, current loss: 1513.6515669966193\n",
            "Epoch: 18, current loss: 1603.7138244936568\n",
            "Epoch: 19, current loss: 1511.709157751103\n",
            "Epoch: 20, current loss: 1603.7612098536606\n",
            "Epoch: 21, current loss: 1511.7689370756063\n",
            "Epoch: 22, current loss: 1603.8119885350527\n",
            "Epoch: 23, current loss: 1493.8300438310646\n",
            "Epoch: 24, current loss: 1581.8629791499945\n",
            "Epoch: 25, current loss: 1487.8897126068719\n",
            "Epoch: 26, current loss: 1509.9150480120134\n",
            "Epoch: 27, current loss: 1417.9424636856802\n",
            "Epoch: 28, current loss: 1341.9610023333596\n",
            "Epoch: 29, current loss: 1279.9819265373103\n",
            "Epoch: 30, current loss: 1129.9963363747072\n",
            "Epoch: 31, current loss: 966.0082276497584\n",
            "Epoch: 32, current loss: 694.0134816376393\n",
            "Epoch: 33, current loss: 470.01456981319933\n",
            "Epoch: 34, current loss: 352.01269065447235\n",
            "Epoch: 35, current loss: 302.01031869041924\n",
            "Epoch: 36, current loss: 278.0080186047288\n",
            "Epoch: 37, current loss: 274.0058801248776\n",
            "Epoch: 38, current loss: 268.0038951309353\n",
            "Epoch: 39, current loss: 256.00205347247083\n",
            "Epoch: 40, current loss: 248.00034215184374\n",
            "Epoch: 41, current loss: 237.9987503433323\n",
            "Epoch: 42, current loss: 229.99726744005864\n",
            "Epoch: 43, current loss: 225.99588503327345\n",
            "Epoch: 44, current loss: 219.99459671909344\n",
            "Epoch: 45, current loss: 217.9933954435712\n",
            "Epoch: 46, current loss: 213.99227508907884\n",
            "Epoch: 47, current loss: 203.99122913909792\n",
            "Epoch: 48, current loss: 197.99025242803305\n",
            "Epoch: 49, current loss: 201.9893394105384\n",
            "Epoch: 50, current loss: 197.988486485528\n",
            "Epoch: 51, current loss: 195.9876898591439\n",
            "Epoch: 52, current loss: 189.98694586274735\n",
            "Epoch: 53, current loss: 191.98625056708133\n",
            "Epoch: 54, current loss: 187.9856007275129\n",
            "Epoch: 55, current loss: 187.98499347371987\n",
            "Epoch: 56, current loss: 181.9844259796945\n",
            "Epoch: 57, current loss: 179.98389570517338\n",
            "Epoch: 58, current loss: 173.98340025726046\n",
            "Epoch: 59, current loss: 167.98293712610914\n",
            "Epoch: 60, current loss: 167.98250433022676\n",
            "Epoch: 61, current loss: 167.98210004503795\n",
            "Epoch: 62, current loss: 165.981722559175\n",
            "Epoch: 63, current loss: 159.98137021941054\n",
            "Epoch: 64, current loss: 157.98104132329277\n",
            "Epoch: 65, current loss: 157.98073442354192\n",
            "Epoch: 66, current loss: 153.98044811957993\n",
            "Epoch: 67, current loss: 153.98018114413986\n",
            "Test set accuracy: 85.5\n"
          ]
        }
      ],
      "source": [
        "# Set up the classifier\n",
        "learning_rate = 0.0001\n",
        "\n",
        "reguliser_dampening = 3\n",
        "\n",
        "decay = 0.000988\n",
        "\n",
        "first_model = model(learning_rate, reguliser_dampening, weights_array,decay,100)\n",
        "\n",
        "# Train on all the non-test data\n",
        "first_model.fit(X_train, y_train)\n",
        "\n",
        "# Run prediction on the test set\n",
        "test_accuracy = first_model.score(X_test,y_test)\n",
        "#np.sum(model.predict(X_test)==y_test)/len(y_test)\n",
        "\n",
        "print(f\"Test set accuracy: {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98224b88",
      "metadata": {
        "id": "98224b88"
      },
      "source": [
        "# Qualitative analysis \n",
        "In the table above we can see that the words with the most negative scores tend to be negative words such as *bad* or *worst* or *boring*. Positive words such as *best* or *great* received very positive scores as expected. However, words such as *plot* and *script* which are not enherently negative also received very negative scores. This could bias the model to classify reviews as negative if they take up these words (and not a lot of other words for instance). \n",
        "\n",
        "My hyperparameters were learning rate, reguliser dampening, decay and the weights. I fixed the starting weights at a seed that appeared to be a good guess right from the start (as I got a fairly high accuracy even when the loss was never going down). The learning rate, reguliser dampening and decay I went with in the end are chosen based on the hyperparameter search conducted in the last section of this assignment. \n",
        "\n",
        "All in all, with a training set accuracy of about 96% and a test set accuracy of 85.5%, the model successfully learned to classify film reviews as positive or negative. Room for improvement exists for instance when it comes to speed; it takes a long time to run the model, potentially because I used lists quite frequently instead of faster data types. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Assignment_1_machine_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1e0786ed",
        "c8d8be07",
        "a4cd61c4",
        "303d1c8d"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}